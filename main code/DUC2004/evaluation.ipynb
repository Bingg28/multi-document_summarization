{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding:utf-8 -*-import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "import sys\n",
    "import io\n",
    "\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "\n",
    "from nltk import tokenize\n",
    "\n",
    "import math\n",
    "\n",
    "from nltk.tree import Tree\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from pythonrouge.pythonrouge import Pythonrouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取（dirindex_path.txt）文件。\n",
    "\n",
    "dir_index_path = {}\n",
    "f_dirpath = open('dirindex_path.txt', 'r', encoding='utf-8')\n",
    "for sents in f_dirpath:\n",
    "    sents = sents.strip('\\n')\n",
    "    item1 = sents.split(':')\n",
    "    dir_index_path[int(item1[0])] = item1[1]\n",
    "f_dirpath.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取（docindex_path.txt）文件。\n",
    "\n",
    "doc_index_path = {}\n",
    "f_docpath = open('docindex_path.txt', 'r', encoding='utf-8')\n",
    "for sents in f_docpath:\n",
    "    sents = sents.strip('\\n')\n",
    "    item1 = sents.split(':')\n",
    "    if int(item1[0]) not in doc_index_path:\n",
    "        doc_index_path[int(item1[0])] = {}\n",
    "        doc_index_path[int(item1[0])][int(item1[1])] = item1[2]\n",
    "    else:\n",
    "        doc_index_path[int(item1[0])][int(item1[1])] = item1[2]\n",
    "f_docpath.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clufinal_ans_alldir = []\n",
    "summarys_alldir = []\n",
    "\n",
    "for dir_i in range(len(dir_index_path)):\n",
    "    # 每个文件夹\n",
    "    \n",
    "    #(tf2_py38环境下)\n",
    "    %run clustering-summarization-run-final.ipynb\n",
    "    #%run clustering-summarization-run-CS.ipynb\n",
    "    \n",
    "    summarys_alldir.append(summarys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path1 = './DUC-2004-docs-references/reference'\n",
    "files1 = os.listdir(path1)\n",
    "reference_summarys_dic = {}\n",
    "for file1 in files1:\n",
    "    if file1 == '.DS_Store':\n",
    "        continue\n",
    "    reference_one = []\n",
    "    path1_detail = path1 + \"/\" + file1\n",
    "    f_path1_detail = open(path1_detail, 'r', encoding='utf-8')\n",
    "    for lines in f_path1_detail:\n",
    "        lines = lines.strip('\\n')\n",
    "        reference_one.append(lines)\n",
    "    dir_index = file1[4:-15]\n",
    "    if dir_index not in reference_summarys_dic:\n",
    "        reference_summarys_dic[dir_index] = []\n",
    "        reference_summarys_dic[dir_index].append(reference_one)\n",
    "    else:\n",
    "        reference_summarys_dic[dir_index].append(reference_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_summarys = []\n",
    "for dir_i in range(len(dir_index_path)):\n",
    "    reference_summarys.append(reference_summarys_dic[dir_index_path[dir_i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 输出所有文件夹的Rouge打分。\n",
    "    \n",
    "rouge = Pythonrouge(summary_file_exist=False,\n",
    "                    summary=summarys_alldir, reference=reference_summarys,\n",
    "                    n_gram=2, ROUGE_SU4=True, ROUGE_L=True,\n",
    "                    recall_only=True, stemming=True, stopwords=False,\n",
    "                    word_level=False, length_limit=True, length=665,\n",
    "                    use_cf=False, cf=95, scoring_formula='average',\n",
    "                    resampling=True, samples=1000, favor=True, p=0.5)\n",
    "\n",
    "score = rouge.calc_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_py38",
   "language": "python",
   "name": "tf2_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
