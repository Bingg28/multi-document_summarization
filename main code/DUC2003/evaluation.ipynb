{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding:utf-8 -*-import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "import sys\n",
    "import io\n",
    "\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "\n",
    "from nltk import tokenize\n",
    "\n",
    "import math\n",
    "\n",
    "from nltk.tree import Tree\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from pythonrouge.pythonrouge import Pythonrouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dirindex_path.txt file.\n",
    "\n",
    "dir_index_path = {}\n",
    "f_dirpath = open('dirindex_path.txt', 'r', encoding='utf-8')\n",
    "for sents in f_dirpath:\n",
    "    sents = sents.strip('\\n')\n",
    "    item1 = sents.split(':')\n",
    "    dir_index_path[int(item1[0])] = item1[1]\n",
    "f_dirpath.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read docindex_path.txt file\n",
    "\n",
    "doc_index_path = {}\n",
    "f_docpath = open('docindex_path.txt', 'r', encoding='utf-8')\n",
    "for sents in f_docpath:\n",
    "    sents = sents.strip('\\n')\n",
    "    item1 = sents.split(':')\n",
    "    if int(item1[0]) not in doc_index_path:\n",
    "        doc_index_path[int(item1[0])] = {}\n",
    "        doc_index_path[int(item1[0])][int(item1[1])] = item1[2]\n",
    "    else:\n",
    "        doc_index_path[int(item1[0])][int(item1[1])] = item1[2]\n",
    "f_docpath.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clufinal_ans_alldir = []\n",
    "summarys_alldir = []\n",
    "\n",
    "for dir_i in range(len(dir_index_path)):\n",
    "    # each folder\n",
    "    \n",
    "    %run clustering-summarization-run.ipynb\n",
    "    \n",
    "    summarys_alldir.append(summarys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = './references'\n",
    "files1 = os.listdir(path1)\n",
    "reference_summarys_dic = {}\n",
    "for file1 in files1:\n",
    "    if file1 == '.DS_Store':\n",
    "        continue\n",
    "    if os.path.isdir(file1):\n",
    "        print(\"ERROR!!!!!! This file is a folder！！！\")\n",
    "        continue\n",
    "    \n",
    "    reference_one = []\n",
    "    path1_detail = path1 + \"/\" + file1\n",
    "    with open(path1_detail,'r',encoding='utf-8') as foo_file:\n",
    "            soup = BeautifulSoup(foo_file)\n",
    "     \n",
    "    f = soup.get_text()\n",
    "    \n",
    "    item = f.replace('\\n',' ')\n",
    "    item_str = ''\n",
    "    item_str = ' '.join(item.split())\n",
    "    \n",
    "    sents = nltk.sent_tokenize(item_str)\n",
    "    for item0 in sents:\n",
    "        reference_one.append(item0)\n",
    "    \n",
    "    file1_list = file1.split('.')\n",
    "    if file1_list[0][1:] not in reference_summarys_dic:\n",
    "        reference_summarys_dic[file1_list[0][1:]] = []\n",
    "        reference_summarys_dic[file1_list[0][1:]].append(reference_one)\n",
    "    else:\n",
    "        reference_summarys_dic[file1_list[0][1:]].append(reference_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_alldir = []\n",
    "for dir_i in range(len(dir_index_path)):\n",
    "    reference_alldir.append(reference_summarys_dic[dir_index_path[dir_i][1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output average Rouge scores of all folders.\n",
    "    \n",
    "rouge = Pythonrouge(summary_file_exist=False,\n",
    "                    summary=summarys_alldir, reference=reference_alldir,\n",
    "                    n_gram=2, ROUGE_SU4=True, ROUGE_L=True,\n",
    "                    recall_only=True, stemming=True, stopwords=False,\n",
    "                    word_level=True, length_limit=True, length=100,\n",
    "                    use_cf=False, cf=95, scoring_formula='average',\n",
    "                    resampling=True, samples=1000, favor=True, p=0.5)\n",
    "\n",
    "score = rouge.calc_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_py38",
   "language": "python",
   "name": "tf2_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
