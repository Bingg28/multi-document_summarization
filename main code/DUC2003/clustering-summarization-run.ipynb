{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b82f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取每篇论文的句子。\n",
    "\n",
    "sents_papers = []\n",
    "for i in doc_index_path[dir_i]:\n",
    "    sents_papers_one = []\n",
    "    onetext_open1 = 'duc_sents/' + str(dir_i) + '/duc' + str(i) + '.txt'\n",
    "    temp_sentslist = open(onetext_open1,'r' ,encoding='utf-8')\n",
    "    for sents in temp_sentslist:\n",
    "        sents = sents.strip('\\n')\n",
    "        sents_papers_one.append(sents)\n",
    "        \n",
    "    sents_papers_one_dic = {}\n",
    "    for k in range(len(sents_papers_one)):\n",
    "        sents_papers_one_dic[k] = sents_papers_one[k]\n",
    "        \n",
    "    sents_papers.append(sents_papers_one_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43ed6ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_papers_list = []\n",
    "for i in range(len(sents_papers)):\n",
    "    sents_papers_list_one = []\n",
    "    for j in sents_papers[i].keys():\n",
    "        sents_papers_list_one.append(sents_papers[i][j])\n",
    "    sents_papers_list.append(sents_papers_list_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d0b70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embedding:\n",
    "\n",
    "pathdir1 = \"./duc_sents_embeddings-DAN/\" + str(dir_i) + \".npy\"\n",
    "\n",
    "temp_sents_embedding = np.load(pathdir1,allow_pickle=True)\n",
    "sents_embedding_list = temp_sents_embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfe4cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_embeddings_list = []\n",
    "for i in range(len(sents_embedding_list)):\n",
    "    papers_embeddings_list.append(np.mean(sents_embedding_list[i], axis=0))\n",
    "    \n",
    "papers_embeddings = np.array(papers_embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6cfc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_clu_list = [1,3,2]\n",
    "# k-means的k值 = min(不重复文本向量的数量，specified-k)\n",
    "doc_all = []\n",
    "for item in doc_index_path[dir_i]:\n",
    "    doc_all.append(item)\n",
    "parent_children = {}\n",
    "root_clusters = []\n",
    "new_clusters = []\n",
    "new_clusters.append(doc_all)\n",
    "for clu_num in k_clu_list:\n",
    "    temp_clu_num = clu_num\n",
    "    categorys = []\n",
    "    for item in new_clusters:\n",
    "        item_str = [str(x) for x in item]\n",
    "        parent_one = ','.join(item_str)\n",
    "        parent_children[parent_one] = []\n",
    "        papers_embeddings_onelist = []\n",
    "        for d_index in item:\n",
    "            papers_embeddings_onelist.append(papers_embeddings[d_index])\n",
    "        papers_embeddings_one = np.array(papers_embeddings_onelist)\n",
    "        unique_rows = np.vstack(list({tuple(row) for row in papers_embeddings_one}))\n",
    "        if len(unique_rows) == 1:\n",
    "            continue\n",
    "        if len(unique_rows) < temp_clu_num: \n",
    "            km = KMeans(n_clusters=len(unique_rows)).fit(papers_embeddings_one)\n",
    "        else:\n",
    "            km = KMeans(n_clusters=temp_clu_num).fit(papers_embeddings_one)\n",
    "        clusters = km.labels_.tolist()\n",
    "        clusters_set = set()\n",
    "        for item11 in clusters:\n",
    "            clusters_set.add(item11)\n",
    "        docindex_cluindex = {}\n",
    "        for i in range(len(clusters)):\n",
    "            docindex_cluindex[item[i]] = clusters[i]\n",
    "        for i in range(len(clusters_set)):\n",
    "            category_one = []\n",
    "            for j in docindex_cluindex:\n",
    "                if docindex_cluindex[j] == i:\n",
    "                    category_one.append(j)\n",
    "            categorys.append(category_one)\n",
    "            \n",
    "            category_one_str = [str(x) for x in category_one]\n",
    "            child_one = ','.join(category_one_str)\n",
    "            parent_children[parent_one].append(child_one)\n",
    "    root_clusters.append(categorys)    \n",
    "    new_clusters = []\n",
    "    new_clusters = categorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92d3f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_bigsmall(strList):\n",
    "    str_dic = {}\n",
    "    for item in strList:\n",
    "        str_dic[item] = len(item.split(','))\n",
    "    ans_str_dic = sorted(str_dic.items(),key = lambda x:x[1],reverse = True)\n",
    "    new_strList = []\n",
    "    for item in ans_str_dic:\n",
    "        new_strList.append(item[0])\n",
    "    return new_strList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f151c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_seq = []\n",
    "doc_all_str = [str(x) for x in doc_all]\n",
    "root = ','.join(doc_all_str)\n",
    "parent_seq.append(root)\n",
    "child_old = []\n",
    "for item0 in parent_children[root]:\n",
    "    child_old.append(item0)\n",
    "child_old = order_bigsmall(child_old)\n",
    "while len(child_old) != 0:\n",
    "    child_new = []\n",
    "    for item1 in child_old:\n",
    "        item1_list = item1.split(',')\n",
    "        if len(item1_list) == 1:\n",
    "            continue\n",
    "        if item1 in parent_children:\n",
    "            for word1 in parent_children[item1]:\n",
    "                child_new.append(word1)\n",
    "        parent_seq.append(item1)\n",
    "    child_old = []\n",
    "    child_old = order_bigsmall(child_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1469407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a包含b。list类型\n",
    "def np_becontained(a, b):\n",
    "    contain_ornot = 0\n",
    "    for word0 in b:\n",
    "        if word0 not in a:\n",
    "            contain_ornot = 1\n",
    "            break\n",
    "    if contain_ornot == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dd04629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a包含b。\n",
    "def str_becontained(a, b):\n",
    "    a_list = a.split(',')\n",
    "    b_list = b.split(',')\n",
    "    contain_ornot = 0\n",
    "    for word0 in b_list:\n",
    "        if word0 not in a_list:\n",
    "            contain_ornot = 1\n",
    "            break\n",
    "    if contain_ornot == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d775fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a和b是否有交集。有交集则返回True。list类型\n",
    "def root_inter_ornot(a, b):\n",
    "    inter_ornot = 0\n",
    "    for word in a:\n",
    "        if word in b:\n",
    "            inter_ornot = 1\n",
    "            break\n",
    "    if inter_ornot == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df71cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_tofather = {}\n",
    "for item in parent_children:\n",
    "    for c_item in parent_children[item]:\n",
    "        parent_tofather[c_item] = item\n",
    "\n",
    "parent_embedding = {}\n",
    "for item in parent_seq:\n",
    "    temp_papers_embedding = []\n",
    "    temp_doc_list = item.split(',')\n",
    "    for temp_doc in temp_doc_list:\n",
    "        temp_papers_embedding.append(papers_embeddings[int(temp_doc)])\n",
    "    temp_papers_embedding1 = np.array(temp_papers_embedding)\n",
    "    parent_embedding[item] = np.mean(temp_papers_embedding1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f0e0756",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_tobrother = {}\n",
    "for item in parent_seq:\n",
    "    bro_list = []\n",
    "    item_list = item.split(',')\n",
    "    for item1 in range(len(sents_papers)):\n",
    "        if str(item1) not in item_list:\n",
    "            bro_list.append(str(item1))\n",
    "    if len(bro_list) != 0:\n",
    "        parent_tobrother[item] = ','.join(bro_list)\n",
    "        \n",
    "brother_embedding = {}\n",
    "for item in parent_tobrother:\n",
    "    temp_brother_embedding = []\n",
    "    item_bro_list = parent_tobrother[item].split(',')\n",
    "    for bro_doc in item_bro_list:\n",
    "        temp_brother_embedding.append(papers_embeddings[int(bro_doc)])\n",
    "    temp_brother_embedding1 = np.array(temp_brother_embedding)\n",
    "    brother_embedding[item] = np.mean(temp_brother_embedding1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d7a53a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_papers_tolist = []\n",
    "for i in range(len(sents_papers)):\n",
    "    sents_papers_tolist_one = {}\n",
    "    for item in sents_papers[i]:\n",
    "        sents_papers_tolist_one[item] = nltk.word_tokenize(sents_papers[i][item])\n",
    "    sents_papers_tolist.append(sents_papers_tolist_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73364aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_sum_threshold = summary_length\n",
    "summary_sents_index = []\n",
    "selected_words_sum = 0\n",
    "\n",
    "onefinish_ornot = 0\n",
    "while onefinish_ornot != 1:\n",
    "    for item0 in parent_seq:\n",
    "        sents_inparent_score = {}\n",
    "        item0_list = item0.split(',')\n",
    "        doc_list = []\n",
    "        for item1 in item0_list:\n",
    "            doc_list.append(int(item1))\n",
    "\n",
    "        item0_cen = parent_embedding[item0]\n",
    "\n",
    "        for item2 in doc_list:\n",
    "            for item3 in sents_papers[item2]:\n",
    "                cos_simi = cosine_similarity(item0_cen.reshape(1,-1),sents_embedding_list[item2][item3].reshape(1,-1))\n",
    "                cos_simil = cos_simi.tolist()\n",
    "                score1 = cos_simil[0][0]\n",
    "                \n",
    "                score2 = 0\n",
    "                for d_sent in summary_sents_index:\n",
    "                    d_sent_list = d_sent.split(',')\n",
    "                    cos2 = cosine_similarity(sents_embedding_list[int(d_sent_list[0])][int(d_sent_list[1])].reshape(1,-1),sents_embedding_list[item2][item3].reshape(1,-1))\n",
    "                    cos21 = cos2.tolist()\n",
    "                    cos211 = cos21[0][0]\n",
    "                    if cos211 > score2:\n",
    "                        score2 = cos211\n",
    "                \n",
    "                score3 = 0\n",
    "                posi = math.exp((-1)*(item3+1)/(math.pow(len(sents_papers[item2]), 1.0/3)))\n",
    "                score3 = max(0.5, posi)\n",
    "                \n",
    "                score4 = 0\n",
    "                if item0 in parent_tofather:\n",
    "                    cos4 = cosine_similarity(parent_embedding[parent_tofather[item0]].reshape(1,-1),sents_embedding_list[item2][item3].reshape(1,-1))\n",
    "                    cos_simi4 = cos4.tolist()\n",
    "                    score4 = cos_simi4[0][0]\n",
    "                    \n",
    "                score5 = 0\n",
    "                if item0 in brother_embedding:\n",
    "                    cos5 = cosine_similarity(brother_embedding[item0].reshape(1,-1),sents_embedding_list[item2][item3].reshape(1,-1))\n",
    "                    cos_simi5 = cos5.tolist()\n",
    "                    score5 = cos_simi5[0][0]\n",
    "                    \n",
    "                sents_inparent_score[str(item2) + \",\" + str(item3)] = 0.9*score1 + 0.1*(1-score5)\n",
    "\n",
    "        index_max = 0\n",
    "        ans_sents_inparent_score = sorted(sents_inparent_score.items(),key = lambda x:x[1],reverse = True)\n",
    "        if ans_sents_inparent_score[0][1] == 0:\n",
    "            continue\n",
    "        for k in range(len(ans_sents_inparent_score)):\n",
    "            if ans_sents_inparent_score[k][0] not in summary_sents_index:\n",
    "                index_max = k\n",
    "                break\n",
    "        if ans_sents_inparent_score[index_max][1] != 0:\n",
    "            summary_sents_index.append(ans_sents_inparent_score[index_max][0])\n",
    "            temp_sent_index = ans_sents_inparent_score[index_max][0].split(',')\n",
    "            selected_words_sum += len(sents_papers_tolist[int(temp_sent_index[0])][int(temp_sent_index[1])]) - 1\n",
    "            if selected_words_sum > words_sum_threshold:\n",
    "                onefinish_ornot = 1\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26f2a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成summary\n",
    "\n",
    "summarys = []\n",
    "for item in summary_sents_index:\n",
    "    item_list = item.split(',')\n",
    "    summarys.append(sents_papers[int(item_list[0])][int(item_list[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ce6636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_py38",
   "language": "python",
   "name": "tf2_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
